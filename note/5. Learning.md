- Machine Learning 机器学习
	机器学习为计算机提供数据，而不是明确的指令。利用这些数据，计算机学会识别模式，并能够自己执行任务。
- Supervised Learning 监督学习
	监督学习是一项任务，计算机根据输入-输出配对的数据集，学习一个将输入映射到输出的函数。
	监督学习下有多种任务，其中之一是**Classification**(分类)。这是一项任务，函数将输入映射到离散的输出。
	- Nearest-Neighbor Classification 最近邻分类
		给定一个输入，选择与该输入最近的数据点的类别。
		- k-nearest-neighbors classification K-近邻算法
			给定一个输入，从与该输入最接近的k个数据点中选择最常见的类别。
	- Perceptron Learning 感知器学习
		与最近邻分类策略相反，另一种处理分类问题的方法是将数据作为一个整体来看待，并试图创建一个决策边界。
		- Weight Vector $\vec w: (w_0, w_1, w_2, ...)$
		- Input Vector $\vec x: (1, x_1, x_2, ...)$
		- $\vec w \cdot \vec x: (w_0 + w_1x_1 + w_2x_2 + ...)$
		- $$hypothesis\ function\ 假设函数：h(x_1,x_2)=\begin{cases}  
					1, & \text{if }\vec w \cdot \vec x\geqslant 0\\  
					0, & otherwiese
					\end{cases}$$
		- 这类函数的问题是它不能表达不确定性，因为它只能等于0或等于1，它采用的是**hard threshold**（硬阈值）。绕过这个问题的方法是使用Logistic函数，它采用的是**soft threshold**（软阈值）。
		- Perceptron Learning Rule 感知器学习规则
			给定数据点（x，y），根据以下情况更新每个权重:
			- $w_i = w_i + \alpha(y - h_w(x)) \times x_i$，即$w_i = w_i + α(actual\ value - estimate) × x_i$
	- Support Vector Machines 支持向量机
		除了最近邻和线性回归之外，另一种分类方法是支持向量机。这种方法在分离数据时，在决策边界附近使用一个额外的向量（支持向量）来做出最佳决策。
		- 所有的决策边界都能发挥作用，它们在分离数据时没有任何错误。然而，它们是否同样好？最左边的两个决策界线与一些观察值非常接近。这意味着，一个新的数据点如果与一组数据只有微小的差别，就会被错误地归为另一组。与此相反，最右边的决策边界与每一组保持最大的距离，从而给其中的变化留出最大的余地。这种类型的边界，与它所分离的两组的距离越远越好，被称为**Maximum Margin Separator**（最大边际分离器）。![[source/5-1.png]]
		- 支持向量机的另一个好处是，它们可以表示两个以上维度的决策边界，以及非线性决策边界![[source/5-2.png]]
	- Regression 回归
		回归是一项监督学习任务，该函数将一个输入点映射到一个连续值，即某个实数。这与分类不同，分类问题是将输入映射到离散的值（0/1）。
		- Loss Functions 损失函数
			损失函数是量化决策规则所损失的效用的一种方式。预测越不准确，损失就越大。
			- 对于分类问题，我们可以使用0-1损失函数。
				```
				L(actual, predicted):
					0 if actual = predicted
					1 otherwise
				```
			- $L_1$和$L_2$损失函数可以在预测一个连续值时使用。在这种情况下，我们感兴趣的是量化每个预测值与观察值的差异程度。我们通过取绝对值或观察值的平方值减去预测值（即预测值离观察值有多远）来做到这一点。
				- $L_1: L(actual, predicted) = |actual - predicted|$
				- $L_2: L(actual, predicted) = (actual - predicted)²$
			- 人们可以选择最适合自己目标的损失函数。L₂对异常值的惩罚比L₁更严厉，因为它对差异进行了平方。L₁可以通过对每个观察点到回归线上的预测点的距离进行求和来实现。
	- Overfitting 过拟合
		过拟合是指一个模型对训练数据的拟合非常好，以至于它不能泛化到其他数据集。在这个意义上，损失函数是一把双刃剑。
	- Regularization 正则化
		正则化是对更复杂的假设进行惩罚的过程，以有利于更简单、更普遍的假设。我们使用正则化来避免过拟合。
		- 在正则化中，我们通过将假设函数h的损失和其复杂性的衡量标准相加来估计其成本。
			- $cost(h) = loss(h) + λcomplexity(h)$
			- $Lambda(\lambda)$是一个常数，我们可以用它来调节我们的成本函数中对复杂性的惩罚力度。$λ$越高，复杂性的代价就越大。
		- 检验我们是否过度拟合模型的一种方法是**Holdout Cross Validation**（保持交叉验证）。在这种技术中，我们把所有的数据分成两部分：一个**training set**（训练集）和一个**test set**（测试集）。我们在训练集上运行学习算法，然后看它对测试集中的数据的预测效果如何。这里的想法是，通过对训练中未使用的数据进行测试，我们可以衡量学习的普遍性如何。
		- 保持交叉验证的缺点是，我们不能在一半的数据上训练模型，因为它是用于评估的。处理这个问题的方法是使用**_k_-Fold Cross-Validation**（K-Fold 交叉验证）。在这个过程中，我们把数据分成k个集合。我们运行训练k次，每次撇开一个数据集并将其作为测试集。我们最终对我们的模型进行了k次不同的评估，我们可以对这些评估进行平均，并在不损失任何数据的情况下获得对我们的模型泛化的估计。
- Reinforcement Learning 强化学习
	强化学习是机器学习的另一种方法，在每次行动之后，代理人都会得到奖励或惩罚（一个正数或一个负数）形式的反馈。
	学习过程由环境向代理提供状态开始。然后，代理人对该状态执行一个行动。基于这个行动，环境将返回一个状态和一个奖励给代理，奖励可以是积极的，使行为在未来更有可能发生，或者是消极的（即惩罚），使行为在未来更不可能发生。
	- Markov Decision Processes 马尔科夫决策过程
		强化学习可以被看作是一个马尔可夫决策过程，具有以下特性：
		- Set of states $S$ 一系列状态
		- Set of actions $Actions(S)$ 状态的一系列行为
		- Transition model $P(s’ | s, a)$ 过渡模型
		- Reward function $R(s, a, s’)$ 奖励函数
	- Q-Learning Q学习
		Q-Learning是强化学习的一个模型，其中一个函数$Q(s, a)$输出对在状态$s$下采取行动$a$的价值估计。
		该模型开始时，所有的估计值都等于0（$Q(s,a) = 0$，适用于所有的$s$，$a$）。当一个行动被采取并收到奖励时，该函数做两件事。
		1. 根据当前的奖励和预期的未来奖励估计$Q(s,a)$的值，以及
		2. 更新$Q(s,a)$以考虑到旧的估计值和新的估计值。这给了我们一个能够改进其过去知识的算法，而无需从头开始。
			- $Q(s, a) ⟵ Q(s, a) + α(new\ value\ estimate - Q(s, a))$
			- $Q(s, a)$的更新值等于$Q(s, a)$的前一个值，再加上一些更新值。这个值被确定为新值和旧值之间的差值，乘以$α$，一个学习系数。当α=1时，新的估计值只是覆盖了旧的估计值。当α=0时，估计值就不会被更新。通过提高和降低$α$，我们可以确定以前的知识被新估计值更新的速度。
		- 新的价值估计可以表示为奖励($r$)和未来奖励估计之和。为了得到未来的奖赏估计，我们考虑采取最后一个行动后得到的新状态，并加上在这个新状态下将带来最高奖赏的行动的估计。这样，我们在状态$s$中进行行动$a$的效用不仅由它得到的奖励来估计，而且还由下一步的预期效用来估计。未来奖励的估计值有时会出现一个系数$\gamma$，控制未来奖励的价值大小。我们最终得到以下方程式：
			- $Q(s, a) ⟵ Q(s, a) + α((r+\gamma max_{a'}Q(s',a')) - Q(s, a))$
		- **Greedy Decision-Making**（贪婪决策算法）完全不考虑未来的估计奖励，而总是选择在当前状态$s$下具有最高$Q(s, a)$的行动$a$。
			- 这使我们要讨论**Explore vs. Exploit**（探索与利用）的权衡。贪婪的算法总是利用，采取已经确定的行动，带来好的结果。然而，它将始终遵循相同的路径来解决问题，从未找到更好的路径。另一方面，探索意味着算法在通往目标的路上可能会使用以前未曾探索过的路线，使其能够在途中发现更有效的解决方案。
			- 为了实现探索和利用的概念，我们可以使用$ε(epsilon)$贪婪算法。在这种类型的算法中，我们将$ε$设定为等于我们想要随机移动的频率。在概率为$1-ε$的情况下，算法会选择最好的移动（利用）。在概率为$ε$的情况下，算法会选择一个随机移动（探索）。
		- 训练强化学习模型的另一种方法是，不是在每一步都给予反馈，而是在整个过程结束时给予反馈。
		- 当一个游戏有多种状态和可能的行动时，这种方法在计算上变得更加苛刻，为每一种可能的状态下的每一个可能的行动生成一个估计值是不可行的。在这种情况下，我们可以使用一个函数近似，它允许我们使用各种其他特征来近似$Q(s, a)$，而不是为每个状态-动作对存储一个值。这样，算法就能够识别哪些动作足够相似，因此它们的估计值也应该是相似的，并在决策中使用这一启发式方法。
- Unsupervised Learning 无监督学习
	- Clustering 聚类
		聚类是一项无监督的学习任务，它将输入的数据组织成组，使类似的对象最终出现在同一组中。
		- k-means Clustering （K-Means 聚类）
			k-means聚类是一种执行聚类任务的算法。它将所有数据点映射到一个空间中，然后在空间中随机放置k个聚类中心（由程序员决定多少个；这就是我们在左边看到的起始状态）。每个聚类中心只是空间中的一个点。然后，每个聚类被分配到所有与它的中心最接近的点，而不是任何其他中心（这是中间的图片）。然后，在一个迭代的过程中，集群中心移动到所有这些点的中间（右边的状态），然后点又被重新分配到现在中心离它们最近的集群。当重复这个过程后，每个点仍然在它之前所在的集群中，我们就达到了一个平衡点，算法就结束了，留给我们的是在各集群之间划分的点。
![[5-3.png]]