- Neural Networks 神经网络
	人工只能神经网络的灵感来自于神经科学。在大脑中，神经元是相互连接的细胞，形成网络。每个神经元都能够接收和发送电信号。一旦神经元收到的电输入越过某个阈值，神经元就会激活，从而向前发送其电信号。
	**Artificial Neural Network**（人工神经网络）是一种受生物神经网络启发的学习数学模型。人工神经网络建立数学函数模型，根据网络的结构和参数将输入映射到输出。在人工神经网络中，网络的结构是通过对数据进行训练而形成的。
	当在人工智能中实现时，每个神经元的并联是一个与其他单元相连的**units**（单元）。例如，像上一节一样，人工智能可能将两个输入$x₁$和$x₂$映射到今天是否会下雨。上一节，我们为这个假设函数提出了如下形式：$h(x₁, x₂) = w₀ + w₁x₁ + w₂x₂$，其中$w₁$和$w₂$是修改输入的权重，$w₀$是一个常数，也叫**bias**（偏置），修改整个表达式的值。
	- Activation Functions 激活函数
		为了使用假设函数来决定它是否下雨，我们需要根据它产生的值来创建某种阈值。
		一种方法是使用step function（阶梯函数），在达到某个阈值之前给出0，达到阈值之后给出1。
		![[source/6-1.png]]
		另一种方法是用logistic function（逻辑函数），它的输出是0到1的任何实数，从而表达了对其判断的分级置信度。
		![[source/6-2.png]]
		另一个可能的功能是Rectified Linear Unit（整流线性单元——ReLU），它允许输出为任何正值。如果数值是负的，ReLU将其设置为0。		
		![[source/6-3.png]]
		无论我们选择使用哪种函数，除了偏置之外，输入还被权重修改，而这些权重的总和被传递给激活函数。这对简单的神经网络来说是正确的。
	- Neural Network Structure 神经网络结构
		神经网络可以被认为是上述想法的表示，其中一个函数将输入加起来产生一个输出。
		![[source/6-4.png]]
		左边的两个白色单元是输入，右边的单元是输出。输入通过加权边与输出相连。为了做出决定，输出单元将输入与它们的权重相乘，再加上偏置（$w₀$），并使用激活函数$g$来决定输出。
	- Gradient Descent 梯度下降算法
		梯度下降是一种在训练神经网络时最小化损失的算法。如前所述，神经网络能够从数据中推断出关于网络本身结构的知识。而到目前为止，我们定义了不同的权重，神经网络允许我们根据训练数据来计算这些权重。为了做到这一点，我们使用梯度下降算法，其工作方式如下：
		- Start with a random choice of weights. This is our naive starting place, where we don’t know how much we should weight each input.从一个随机的权重选择开始。
		- Repeat:
			- Calculate the gradient based on all data points that will lead to decreasing loss. Ultimately, the gradient is a vector (a sequence of numbers).根据所有的数据点计算梯度，会导致损失递减。最终，梯度是一个矢量（一串数字）。
			- Update weights according to the gradient.根据梯度来更新权重。
		这种算法的问题是，它需要根据所有的数据点来计算梯度，这在计算上是有成本的。有多种方法可以使这种成本最小化。例如，在**Stochastic Gradient Descent**（随机梯度下降算法）中，梯度是根据随机选择的一个点来计算的。这种梯度可能是相当不准确的，这导致了**Mini-Batch Gradient Descent**（小批量梯度下降算法），它基于随机选择的几个点来计算梯度，从而在计算成本和准确性之间找到一个折衷。通常情况下，这些解决方案没有一个是完美的，在不同的情况下可能采用不同的解决方案。
		到目前为止，我们的神经网络依靠的是**perceptron**（感知器）输出单元。这些单元只能够学习线性决策边界，用一条直线来分离数据。也就是说，基于一个线性方程，感知器可以将一个输入分类为一种类型或另一种类型（例如，左图）。然而，通常情况下，数据不是线性可分离的（如右图）。在这种情况下，我们转向多层神经网络来对数据进行非线性建模。
		![[source/6-5.png]]
	- Multilayer Neural Networks 多层神经网络
		多层神经网络是一个具有输入层、输出层和至少一个**hidden** layer（隐藏层）的人工神经网络。虽然我们提供输入和输出来训练模型，但我们并不向隐藏层内的单元提供任何数值。第一个隐藏层的每个单元从输入层的每个单元接收一个加权值，对其进行一些操作并输出一个值。这些值中的每一个都经过加权，并进一步传播到下一层，重复这一过程，直到到达输出层。通过隐藏层，有可能对非线性数据进行建模。
		![[source/6-6.png]]
	- Backpropagation 反向传播算法
	  反向传播是用于训练具有隐藏层的神经网络的主要算法。它通过从输出单元的错误开始，计算前一层权重的梯度下降，并重复这一过程，直到达到输入层。在伪代码中，我们可以描述该算法如下：
	  - Calculate error for output layer 计算输出层的误差
	  - For each layer, starting with output layer and moving inwards towards earliest hidden layer: 对于每一层，从输出层开始，向内移动到最早的隐藏层。
		  - Propagate error back one layer. In other words, the current layer that’s being considered sends the errors to the preceding layer. 将错误传播回一层。换句话说，正在考虑的当前层将错误发送到前面的层。
		  - Update weights. 更新权重。
		这可以扩展到任何数量的隐藏层，创建**deep neural networks**（深度神经网络），也就是拥有一个以上隐藏层的神经网络。
- Overfitting 过拟合
	过拟合是指对训练数据的建模过于紧密，从而无法对新数据进行归纳的情况。应对过拟合的一种方法是**dropout**。在这种技术中，我们在学习阶段临时删除随机选择的单元。通过这种方式，我们试图防止对网络中任何一个单元的过度依赖。在整个训练过程中，神经网络会呈现出不同的形式，每次都会放弃一些其他单元，然后再次使用它们。
	![[source/6-7.png]]
	请注意，训练结束后，整个神经网络将被再次使用。
- TensorFlow
	就像在python中经常发生的那样，多个库已经有了使用反向传播算法的神经网络的实现，而TensorFlow就是这样一个库。
- Computer Vision (CV) 计算机视觉
	计算机视觉包括分析和理解数字图像的不同计算方法，它通常使用神经网络实现。例如，当社交媒体采用人脸识别来自动标记图片中的人时，就会用到计算机视觉。其他的例子是手写识别和自动驾驶汽车。
	图像由像素组成，像素由三个值表示，范围从0到255，一个代表红色，一个代表绿色，一个代表蓝色。这些值通常用RGB这个缩写来表示。我们可以用它来创建一个神经网络，每个像素中的每个颜色值都是一个输入，我们有一些隐藏层，而输出是一些单元，告诉我们图像中显示的是什么。然而，这种方法有几个缺点。首先，通过将图像分解成像素和它们的颜色值，我们不能使用图像的结构作为辅助。也就是说，作为人类，如果我们看到一张脸的一部分，我们就知道要看到脸的其余部分，这就加快了计算。我们希望能够在我们的神经网络中使用一个类似的优势。第二，输入的数量非常大，这意味着我们将不得不计算大量的权重。
	- Image Convolution 图像卷积
		图像卷积是应用一个过滤器，将图像的每个像素值加到它的邻居身上，根据内核矩阵加权。这样做改变了图像，可以帮助神经网络处理它。
		![[source/6-8.png]]
		不同的内核可以实现不同的任务。对于边缘检测，通常使用以下内核：
		![[source/6-9.png]]
		这里的想法是，当像素与它的所有邻居相似时，它们应该相互抵消，给出一个0的值。因此，像素越相似，图像的部分就越暗，而它们越不同，就越亮。
		尽管如此，由于作为神经网络输入的像素数量，在神经网络中处理图像的计算成本还是很高。另一种方法是池化（**Pooling**），即通过从输入的区域中取样来减少输入的大小。彼此相邻的像素属于图像中的同一区域，这意味着它们很可能是相似的。因此，我们可以用一个像素来代表整个区域。一种方法是使用**Max-Pooling**，所选的像素是同一区域内所有其他像素中价值最高的一个。例如，如果我们把左边的正方形（下图）分成四个2X2的正方形，通过从这个输入的最大池，我们得到右边的小正方形。
		![[source/6-10.png]]
- Convolutional Neural Networks (CNN) 卷积神经网络
	卷积神经网络是一种使用卷积的神经网络，通常用于分析图像。它首先应用过滤器，使用不同的内核帮助提炼出图像的一些特征。这些过滤器可以以与神经网络中其他权重相同的方式进行改进，根据输出的误差调整其内核。然后，将得到的图像进行池化，之后将这些像素作为输入送入传统的神经网络（这个过程称为**flattening**扁平化）。
	![[source/6-11.png]]
	卷积和池化的步骤可以重复多次，以提取更多的特征并减少神经网络的输入大小。这些过程的好处之一是，通过卷积和池化，神经网络对变化的敏感度降低。也就是说，如果同一张照片从稍微不同的角度拍摄，卷积神经网络的输入将是相似的，而如果没有卷积和池化，每张照片的输入将有很大的不同。
- Recurrent Neural Networks (RNN)循环神经网络
	**Feed-Forward Neural Networks**（前馈神经网络）是我们迄今为止讨论过的神经网络的类型，输入数据被提供给网络，最终产生一些输出。下面可以看到前馈神经网络的工作原理图。
	![[source/6-12.png]]
	与此相反，**Recurrent Neural Networks**（循环神经网络）由一个非线性结构组成，网络使用自己的输出作为输入。例如，微软的*captionbot*能够用句子中的单词来描述图像的内容。这与分类不同的是，输出可以根据图像的属性而有不同的长度。虽然前馈神经网络无法改变输出的数量，但由于其结构，循环神经网络能够做到这一点。在语言输出任务中，一个网络将处理输入以产生一个输出，然后从这一点开始继续处理，产生另一个输出，并根据需要不断重复。
	![[source/6-13.png]]
	递归神经网络在网络处理序列而不是单个对象的情况下很有帮助。上面，神经网络需要产生一个单词序列。然而，同样的原则也可以应用于分析视频文件，它由一连串的图像组成，或者在翻译任务中，一连串的输入（源语言的词）被处理以产生一连串的输出（目标语言的词）。